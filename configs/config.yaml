train:
  batch_size: 64
  epochs: 5
  log_interval: 100
  generator_lr: 1e-4
  discriminator_adam_lr: 1e-5
  num_workers: 4
  # For SGD optimizer
  use_sgd: true
  discriminator_sgd_lr: 2e-4 # It's slightly slower and needs higher learning rate
  # for continuing training
  load_checkpoint: # change from None to checkpoint path to continue training
  more_epochs: 20 # How many more epochs to train

logger:
  checkpoint_path: "checkpoints/"
  chart_path: "tmp/"

system:
  device: "cuda:0"
  seed: 23

data:
  channels: 1
  size: 1000

discriminator:
  neurons: [64, 128, 256, 512]
  beta: 0.5

generator:
  noise_size: 1000
  beta: 0.5

project: "ECG GAN"
run_date: ${now:%Y_%m_%d_%H_%M}
run_name: "ECG_GAN_${run_date}"
